{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6755575a-2244-4d2b-9177-d920a511f970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sr/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-09 17:22:22 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MathVista evaluation with VERL and confidence tracing.\n",
    "\n",
    "This script runs Qwen3-VL-4B-Thinking on the MathVista dataset with vLLM\n",
    "(0.11.0) and logs inference trajectories to Weights & Biases. It captures\n",
    "both logit-based confidence and a simple relative confidence heuristic to\n",
    "study how model \"state\" evolves during reasoning.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from transformers import AutoProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "PHASES = [\n",
    "    \"read\",\n",
    "    \"explore\",\n",
    "    \"insight\",\n",
    "    \"solve\",\n",
    "    \"verify\",\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ConfidenceSummary:\n",
    "    absolute: float\n",
    "    relative: float\n",
    "    phase_curve: List[Tuple[str, float]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SampleResult:\n",
    "    question_id: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    prediction: str\n",
    "    confidence: ConfidenceSummary\n",
    "    trajectory: List[Dict[str, float]]\n",
    "\n",
    "\n",
    "def build_messages(question: str, image: Image.Image) -> List[Dict]:\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a careful math and vision tutor. Read the question and image, \"\n",
    "            \"think step by step, and output a concise final answer.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": f\"{question}\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_confidence(\n",
    "    token_ids: List[int],\n",
    "    logprob_trace: List[Dict],\n",
    ") -> ConfidenceSummary:\n",
    "    \"\"\"Derive absolute and relative confidence from vLLM logprobs.\"\"\"\n",
    "\n",
    "    token_confidences: List[float] = []\n",
    "    margin_confidences: List[float] = []\n",
    "\n",
    "    for step, token_id in enumerate(token_ids):\n",
    "        step_logprobs = logprob_trace[step] if step < len(logprob_trace) else {}\n",
    "        probs: List[float] = []\n",
    "        target_prob: Optional[float] = None\n",
    "\n",
    "        for entry in step_logprobs.values():\n",
    "            if isinstance(entry, (float, int)):\n",
    "                logprob = float(entry)\n",
    "                entry_token_id = None\n",
    "            else:\n",
    "                logprob = getattr(entry, \"logprob\", None)\n",
    "                entry_token_id = getattr(entry, \"token_id\", None)\n",
    "\n",
    "            if logprob is None:\n",
    "                continue\n",
    "\n",
    "            prob = float(torch.exp(torch.tensor(logprob)).item())\n",
    "            probs.append(prob)\n",
    "            if entry_token_id == token_id:\n",
    "                target_prob = prob\n",
    "\n",
    "        if target_prob is None and probs:\n",
    "            target_prob = probs[0]\n",
    "\n",
    "        token_confidences.append(target_prob or 0.0)\n",
    "        if len(probs) >= 2:\n",
    "            top_two = sorted(probs, reverse=True)[:2]\n",
    "            margin_confidences.append(top_two[0] - top_two[1])\n",
    "        elif probs:\n",
    "            margin_confidences.append(probs[0])\n",
    "        else:\n",
    "            margin_confidences.append(0.0)\n",
    "\n",
    "    avg_prob = sum(token_confidences) / max(len(token_confidences), 1)\n",
    "    absolute = float(avg_prob)\n",
    "    relative = float(sum(margin_confidences) / max(len(margin_confidences), 1))\n",
    "    phase_curve = interpolate_phases(token_confidences)\n",
    "    return ConfidenceSummary(absolute=absolute, relative=relative, phase_curve=phase_curve)\n",
    "\n",
    "\n",
    "def interpolate_phases(token_confidences: List[float]) -> List[Tuple[str, float]]:\n",
    "    if not token_confidences:\n",
    "        return [(phase, 0.0) for phase in PHASES]\n",
    "\n",
    "    min_conf = min(token_confidences)\n",
    "    max_conf = max(token_confidences)\n",
    "    span = max(max_conf - min_conf, 1e-6)\n",
    "    normalized = [(c - min_conf) / span for c in token_confidences]\n",
    "\n",
    "    checkpoints = []\n",
    "    for idx, phase in enumerate(PHASES):\n",
    "        pos = int((idx / max(len(PHASES) - 1, 1)) * (len(normalized) - 1))\n",
    "        checkpoints.append((phase, float(normalized[pos])))\n",
    "    return checkpoints\n",
    "\n",
    "\n",
    "def run_sample(\n",
    "    llm: LLM,\n",
    "    processor,\n",
    "    sample: Dict,\n",
    "    sampling_params: SamplingParams,\n",
    ") -> SampleResult:\n",
    "    image: Image.Image = sample[\"image\"]\n",
    "    messages = build_messages(sample[\"question\"], image)\n",
    "    prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        {\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}},\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "\n",
    "    result = outputs[0].outputs[0]\n",
    "    logprob_trace = result.logprobs or []\n",
    "    confidence = extract_confidence(result.token_ids, logprob_trace)\n",
    "\n",
    "    return SampleResult(\n",
    "        question_id=str(sample.get(\"qid\", sample.get(\"id\", \"unknown\"))),\n",
    "        question=sample[\"question\"],\n",
    "        answer=sample.get(\"answer\", \"\"),\n",
    "        prediction=result.text.strip(),\n",
    "        confidence=confidence,\n",
    "        trajectory=[{\"phase\": phase, \"confidence\": value} for phase, value in confidence.phase_curve],\n",
    "    )\n",
    "\n",
    "\n",
    "def log_to_wandb(table: wandb.Table, result: SampleResult) -> None:\n",
    "    table.add_data(\n",
    "        result.question_id,\n",
    "        result.question,\n",
    "        result.answer,\n",
    "        result.prediction,\n",
    "        result.confidence.absolute,\n",
    "        result.confidence.relative,\n",
    "        result.trajectory,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model_id: str,\n",
    "    split: str,\n",
    "    limit: Optional[int],\n",
    "    batch_size: int,\n",
    "    max_new_tokens: int,\n",
    "    project: str,\n",
    "    run_name: str,\n",
    "    data_dir: Optional[str] = None,\n",
    "    dtype: str = \"bfloat16\",\n",
    "    tensor_parallel_size: int = 1,\n",
    ") -> None:\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "    llm = LLM(\n",
    "        model=model_id,\n",
    "        trust_remote_code=True,\n",
    "        dtype=dtype,\n",
    "        tensor_parallel_size=tensor_parallel_size,\n",
    "    )\n",
    "\n",
    "    if data_dir:\n",
    "        candidate_dir = os.path.join(data_dir, split)\n",
    "        try:\n",
    "            if os.path.isdir(candidate_dir):\n",
    "                loaded = load_from_disk(candidate_dir)\n",
    "            else:\n",
    "                loaded = load_from_disk(data_dir)\n",
    "\n",
    "            dataset = loaded[split] if hasattr(loaded, \"keys\") and split in loaded else loaded\n",
    "        except Exception:\n",
    "            # Fallback to remote if local loading fails\n",
    "            dataset = load_dataset(\"AI-MO/MathVista\", split=split)\n",
    "    else:\n",
    "        dataset = load_dataset(\"AI-MO/MathVista\", data_dir=data_dir, split=split)\n",
    "    if limit:\n",
    "        dataset = dataset.select(range(limit))\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=0.0,\n",
    "        logprobs=10,\n",
    "    )\n",
    "\n",
    "    wandb.init(project=project, name=run_name, config={\n",
    "        \"model_id\": model_id,\n",
    "        \"split\": split,\n",
    "        \"limit\": limit,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "    })\n",
    "\n",
    "    table = wandb.Table(\n",
    "        columns=[\n",
    "            \"question_id\",\n",
    "            \"question\",\n",
    "            \"ground_truth\",\n",
    "            \"prediction\",\n",
    "            \"absolute_confidence\",\n",
    "            \"relative_confidence\",\n",
    "            \"phase_trajectory\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for start in tqdm(range(0, len(dataset), batch_size), desc=\"Evaluating\"):\n",
    "        batch = dataset[start : start + batch_size]\n",
    "        for sample in batch:\n",
    "            result = run_sample(\n",
    "                llm=llm,\n",
    "                processor=processor,\n",
    "                sample=sample,\n",
    "                sampling_params=sampling_params,\n",
    "            )\n",
    "            log_to_wandb(table, result)\n",
    "\n",
    "    wandb.log({\"mathvista_eval\": table})\n",
    "    wandb.finish()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a11e8-a7a2-4352-a0a0-c081b3e630ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"MathVista evaluation with VERL-style logging\")\n",
    "    parser.add_argument(\"--model-id\", default=\"Qwen/Qwen3-VL-4B-Thinking\", help=\"Model identifier\")\n",
    "    parser.add_argument(\"--split\", default=\"testmini\", help=\"Dataset split to evaluate\")\n",
    "    parser.add_argument(\"--limit\", type=int, default=None, help=\"Limit number of samples\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=1, help=\"Batch size for evaluation\")\n",
    "    parser.add_argument(\"--max-new-tokens\", type=int, default=256, help=\"Maximum tokens to generate\")\n",
    "    parser.add_argument(\"--project\", default=\"mathvista-verl\", help=\"Weights & Biases project name\")\n",
    "    parser.add_argument(\"--run-name\", default=\"qwen3-vl-4b-thinking\", help=\"Weights & Biases run name\")\n",
    "    parser.add_argument(\"--data-dir\", default=None, help=\"Optional local path to MathVista data\")\n",
    "    parser.add_argument(\"--dtype\", default=\"bfloat16\", help=\"Computation dtype for vLLM (e.g., float16, bfloat16)\")\n",
    "    parser.add_argument(\"--tensor-parallel-size\", type=int, default=1, help=\"Tensor parallelism for vLLM workers\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "model_id = \"/root/autodl-tmp/model\"\n",
    "split = \"testmini\"\n",
    "limit = None\n",
    "batch_size = 1\n",
    "max_new_tokens = 1024\n",
    "project = \"StateReasoning\"\n",
    "run_name = \"qwen3-vl-4b-thinking\"\n",
    "data_dir = None\n",
    "dtype = \"bfloat16\"\n",
    "\n",
    "def main() -> None:\n",
    "    args = parse_args()\n",
    "    evaluate(\n",
    "        model_id=\"/root/autodl-tmp/model\",\n",
    "        split=\"testmini\",\n",
    "        limit=None,\n",
    "        batch_size=16,\n",
    "        max_new_tokens=1024,\n",
    "        project=\"StateReasoning\",\n",
    "        run_name=\"qwen3-vl-4b-thinking\",\n",
    "        data_dir=args.data_dir,\n",
    "        dtype=args.dtype,\n",
    "        tensor_parallel_size=args.tensor_parallel_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba2ccb3-5c86-4dd2-bd24-7a421b246ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b74d5e8-6fca-4392-9180-46ec4188ca5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'evalscope/MathVista' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDatasetNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m split = \u001b[33m\"\u001b[39m\u001b[33mtestmini\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 3. 执行加载\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mevalscope/MathVista\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 4. 查看数据\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m数据集结构: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# 通常是 Dataset 或 DatasetDict\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sr/lib/python3.12/site-packages/datasets/load.py:1397\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1392\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1393\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1394\u001b[39m )\n\u001b[32m   1396\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1413\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sr/lib/python3.12/site-packages/datasets/load.py:1137\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1147\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sr/lib/python3.12/site-packages/datasets/load.py:1030\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1028\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1032\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1033\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1034\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1035\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sr/lib/python3.12/site-packages/datasets/load.py:985\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[32m    982\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRevision \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    983\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    987\u001b[39m     api.hf_hub_download(\n\u001b[32m    988\u001b[39m         repo_id=path,\n\u001b[32m    989\u001b[39m         filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    992\u001b[39m         proxies=download_config.proxies,\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[31mDatasetNotFoundError\u001b[39m: Dataset 'evalscope/MathVista' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": [
    "# 1. 导入库\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 2. 定义（或使用默认）数据目录和要加载的分割部分\n",
    "data_dir = \"/root/autodl-tmp/data\"  # 可以自定义，或设为None用默认缓存\n",
    "split = \"testmini\" \n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"AI4Math/MathVista\")\n",
    "\n",
    "# 3. 执行加载\n",
    "dataset = load_dataset(\"AI4Math/MathVista\", data_dir=data_dir, split=split)\n",
    "\n",
    "# 4. 查看数据\n",
    "print(f\"数据集结构: {type(dataset)}\")  # 通常是 Dataset 或 DatasetDict\n",
    "print(f\"示例数量: {len(dataset)}\")\n",
    "print(\"\\n第一条数据示例:\")\n",
    "print(dataset[0])  # 查看第一条数据的内容"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sr",
   "language": "python",
   "name": "sr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
