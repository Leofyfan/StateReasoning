{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb65b50-caea-4e49-983a-f256aaf9e11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sr/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 00:36:57 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from transformers import AutoProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873e936-7461-472c-b008-98a3e90d9b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f10f2a-52e1-4e51-9a89-a288e703b1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400689f6-8bf9-4704-886a-f0cf1e6b2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASES = [\"read\", \"explore\", \"insight\", \"solve\", \"verify\"]\n",
    "\n",
    "@dataclass\n",
    "class ConfidenceSummary:\n",
    "    absolute: float\n",
    "    relative: float\n",
    "    phase_curve: List[Tuple[str, float]]\n",
    "\n",
    "@dataclass\n",
    "class SampleResult:\n",
    "    question_id: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    prediction: str\n",
    "    confidence: ConfidenceSummary\n",
    "    trajectory: List[Dict[str, float]]\n",
    "\n",
    "\n",
    "\n",
    "def extract_confidence(\n",
    "    token_ids: List[int],\n",
    "    logprob_trace: List[Dict],\n",
    ") -> ConfidenceSummary:\n",
    "\n",
    "    token_confidences = []\n",
    "    margin_confidences = []\n",
    "\n",
    "    for step, token_id in enumerate(token_ids):\n",
    "        step_logprobs = logprob_trace[step] if step < len(logprob_trace) else {}\n",
    "        probs = []\n",
    "        target_prob = None\n",
    "\n",
    "        for entry in step_logprobs.values():\n",
    "            if isinstance(entry, (float, int)):\n",
    "                logprob = float(entry)\n",
    "                entry_token_id = None\n",
    "            else:\n",
    "                logprob = getattr(entry, \"logprob\", None)\n",
    "                entry_token_id = getattr(entry, \"token_id\", None)\n",
    "\n",
    "            if logprob is None:\n",
    "                continue\n",
    "\n",
    "            prob = float(torch.exp(torch.tensor(logprob)).item())\n",
    "            probs.append(prob)\n",
    "            if entry_token_id == token_id:\n",
    "                target_prob = prob\n",
    "\n",
    "        if target_prob is None and probs:\n",
    "            target_prob = probs[0]\n",
    "\n",
    "        token_confidences.append(target_prob or 0.0)\n",
    "\n",
    "        if len(probs) >= 2:\n",
    "            top_two = sorted(probs, reverse=True)[:2]\n",
    "            margin_confidences.append(top_two[0] - top_two[1])\n",
    "        elif probs:\n",
    "            margin_confidences.append(probs[0])\n",
    "        else:\n",
    "            margin_confidences.append(0.0)\n",
    "\n",
    "    absolute = float(sum(token_confidences) / max(len(token_confidences), 1))\n",
    "    relative = float(sum(margin_confidences) / max(len(margin_confidences), 1))\n",
    "    phase_curve = interpolate_phases(token_confidences)\n",
    "\n",
    "    return ConfidenceSummary(absolute, relative, phase_curve)\n",
    "\n",
    "\n",
    "def interpolate_phases(token_confidences: List[float]) -> List[Tuple[str, float]]:\n",
    "    if not token_confidences:\n",
    "        return [(phase, 0.0) for phase in PHASES]\n",
    "\n",
    "    mn, mx = min(token_confidences), max(token_confidences)\n",
    "    span = max(mx - mn, 1e-6)\n",
    "\n",
    "    normalized = [(c - mn) / span for c in token_confidences]\n",
    "\n",
    "    checkpoints = []\n",
    "    for idx, phase in enumerate(PHASES):\n",
    "        pos = int((idx / max(len(PHASES)-1, 1)) * (len(normalized)-1))\n",
    "        checkpoints.append((phase, float(normalized[pos])))\n",
    "\n",
    "    return checkpoints\n",
    "\n",
    "def run_sample(llm, processor, sample, sampling_params):\n",
    "    # 从路径加载图像（你当前用的是原始 image 路径，不是 decoded_image）\n",
    "    image = Image.open(\"/root/autodl-tmp/data/\" + sample[\"image\"]).convert(\"RGB\")\n",
    "    \n",
    "    # 构造 OpenAI-style messages（vLLM 内部会自动转换为合法 token）\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": sample[\"query\"]}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # ✅ 关键：用 processor 生成 prompt（包含合法的视觉 token）\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 调用 vLLM（注意：multi_modal_data 传 [image] 列表！）\n",
    "    outputs = llm.generate(\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"multi_modal_data\": {\"image\": [image]}  \n",
    "        },\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "\n",
    "    \n",
    "    result = outputs[0].outputs[0]\n",
    "\n",
    "    \n",
    "    logprob_trace = result.logprobs or []\n",
    "    confidence = extract_confidence(result.token_ids, logprob_trace)\n",
    "\n",
    "    return SampleResult(\n",
    "        question_id=str(sample.get(\"pid\", sample.get(\"id\", \"unknown\"))),\n",
    "        question=sample[\"question\"],\n",
    "        answer=sample.get(\"answer\", \"\"),\n",
    "        prediction=result.text.strip(),\n",
    "        confidence=confidence,\n",
    "        trajectory=[\n",
    "            {\"phase\": phase, \"confidence\": value}\n",
    "            for phase, value in confidence.phase_curve\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "def log_to_wandb(table, result):\n",
    "    table.add_data(\n",
    "        result.question_id,\n",
    "        result.question,\n",
    "        result.answer,\n",
    "        result.prediction,\n",
    "        result.confidence.absolute,\n",
    "        result.confidence.relative,\n",
    "        result.trajectory,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb48af-95ba-4a96-b76a-fee4fd528c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02994098-cf43-4454-a138-12439e0ead26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2accddb0-5e63-4e4b-b636-aaf7b7f41c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/root/autodl-tmp/model\"\n",
    "split = \"testmini\"\n",
    "limit = 16\n",
    "batch_size = 16\n",
    "max_new_tokens = 1024\n",
    "project = \"StateReasoning\"\n",
    "run_name = \"qwen3-vl-4b-thinking\"\n",
    "data_dir = None\n",
    "# V100 (compute capability 7.0) does not support bfloat16; use float16 instead\n",
    "# to avoid engine initialization failure.\n",
    "dtype = \"float16\"\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    llm = LLM(\n",
    "        model=model_id,\n",
    "        trust_remote_code=True,\n",
    "        dtype=dtype,\n",
    "        # Limit context length to fit V100 32GB KV cache (was 262144 needing ~36 GiB)\n",
    "        max_model_len=8192,\n",
    "        # Slightly raise utilization; adjust if OOM\n",
    "        gpu_memory_utilization=0.5,\n",
    "        enable_chunked_prefill=False  # 多模态建议关闭\n",
    "    )\n",
    "\n",
    "\n",
    "    dataset = load_dataset(\"AI4Math/MathVista\", split=split)\n",
    "    if limit:\n",
    "        dataset = dataset.select(range(limit))\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=0.1,\n",
    "        logprobs=10,\n",
    "    )\n",
    "\n",
    "    wandb.init(project=project, name=run_name)\n",
    "\n",
    "    table = wandb.Table(\n",
    "        columns=[\n",
    "            \"question_id\",\n",
    "            \"question\",\n",
    "            \"ground_truth\",\n",
    "            \"prediction\",\n",
    "            \"absolute_confidence\",\n",
    "            \"relative_confidence\",\n",
    "            \"phase_trajectory\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for start in tqdm(range(0, len(dataset), batch_size), desc=\"Evaluating\"):\n",
    "\n",
    "        end = min(start + batch_size, len(dataset))\n",
    "        batch = dataset.select(range(start, end)).to_list()\n",
    "\n",
    "        for sample in batch:\n",
    "            result = run_sample(llm, processor, sample, sampling_params)\n",
    "            log_to_wandb(table, result)\n",
    "\n",
    "    wandb.log({\"mathvista_eval\": table})\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e511ac-a3ad-4849-8ee5-f024caa37c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a89b82-66ad-4519-8126-c01ca6139444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sr",
   "language": "python",
   "name": "sr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
