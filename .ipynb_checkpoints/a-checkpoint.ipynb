{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb65b50-caea-4e49-983a-f256aaf9e11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sr/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 16:46:45 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from transformers import AutoProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5873e936-7461-472c-b008-98a3e90d9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # dataset = load_dataset(\"AI4Math/MathVista\", split=\"testmini\")\n",
    "\n",
    "# # print(len(dataset))\n",
    "\n",
    "# # print(type(dataset))\n",
    "\n",
    "# # print(dataset)\n",
    "\n",
    "\n",
    "# # batch = dataset.select(range(16)).to_list()\n",
    "\n",
    "# # print(len(batch))\n",
    "# # print(type(batch))\n",
    "\n",
    "\n",
    "# # # for start in tqdm(range(0, len(dataset), 1), desc=\"Evaluating\"):\n",
    "# # #     print(type(dataset[start]))\n",
    "# # #     print(dataset[start])\n",
    "\n",
    "# from collections import Counter\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # def print_dataset_stats(dataset, num_samples=100):\n",
    "# #     \"\"\"\n",
    "# #     ÊâìÂç∞ dataset ‰∏≠ÊØè‰∏™Â≠óÊÆµÁöÑÁªüËÆ°‰ø°ÊÅØ„ÄÇ\n",
    "# #     ÂèÇÊï∞:\n",
    "# #         dataset: datasets.Dataset ÂØπË±°\n",
    "# #         num_samples: Áî®‰∫éÊäΩÊ†∑ÁªüËÆ°ÁöÑÊ†∑Êú¨Êï∞ÔºàÈÅøÂÖçÂÖ®ÈáèÊâ´ÊèèÔºåÊèêÂçáÈÄüÂ∫¶Ôºâ\n",
    "# #     \"\"\"\n",
    "# #     total_samples = len(dataset)\n",
    "# #     print(f\"üìä Dataset ÊÄªÊ†∑Êú¨Êï∞: {total_samples}\\n\")\n",
    "    \n",
    "# #     # ÈöèÊú∫ÊàñÈ°∫Â∫èÊäΩÊ†∑ÔºàËøôÈáåÁî®È°∫Â∫èÂâç num_samplesÔºâ\n",
    "# #     sample_indices = range(min(num_samples, total_samples))\n",
    "# #     sampled_data = dataset.select(sample_indices)\n",
    "    \n",
    "# #     for field in dataset.features:\n",
    "# #         print(f\"üîπ Â≠óÊÆµ: '{field}'\")\n",
    "# #         feature_type = dataset.features[field]\n",
    "# #         print(f\"   - Á±ªÂûã: {feature_type}\")\n",
    "        \n",
    "# #         # Ëé∑ÂèñËØ•Â≠óÊÆµÂú®ÊäΩÊ†∑‰∏≠ÁöÑÂÄºÂàóË°®\n",
    "# #         values = [sampled_data[i][field] for i in range(len(sampled_data))]\n",
    "        \n",
    "# #         # ÁªüËÆ°ÈùûÁ©∫ÂÄºÊï∞Èáè\n",
    "# #         non_null_count = sum(1 for v in values if v is not None and v != \"\")\n",
    "# #         null_count = len(values) - non_null_count\n",
    "# #         print(f\"   - ÈùûÁ©∫ÂÄº: {non_null_count} / {len(values)} ({null_count} ‰∏∫Á©∫Êàñ None)\")\n",
    "        \n",
    "# #         # Á±ªÂûãÁâπÂÆöÁªüËÆ°\n",
    "# #         if field == \"decoded_image\" or (hasattr(feature_type, 'dtype') and 'image' in str(feature_type).lower()):\n",
    "# #             # ÂõæÂÉèÂ≠óÊÆµ\n",
    "# #             sizes = []\n",
    "# #             modes = []\n",
    "# #             for v in values:\n",
    "# #                 if v is not None:\n",
    "# #                     if isinstance(v, list):\n",
    "# #                         # Â§öÂõæÊÉÖÂÜµÔºöÂèñÁ¨¨‰∏ÄÂº†\n",
    "# #                         v = v[0] if len(v) > 0 else None\n",
    "# #                     if isinstance(v, Image.Image):\n",
    "# #                         sizes.append(v.size)\n",
    "# #                         modes.append(v.mode)\n",
    "# #             if sizes:\n",
    "# #                 widths, heights = zip(*sizes)\n",
    "# #                 print(f\"   - ÂõæÂÉèÂ∞∫ÂØ∏ (W√óH): min=({min(widths)}, {min(heights)}) max=({max(widths)}, {max(heights)})\")\n",
    "# #                 print(f\"   - ÂõæÂÉèÊ®°ÂºèÁªüËÆ°: {Counter(modes)}\")\n",
    "# #             else:\n",
    "# #                 print(\"   - Êó†ÊúâÊïàÂõæÂÉè\")\n",
    "                \n",
    "# #         elif isinstance(values[0], list) and len(values[0]) > 0 and isinstance(values[0][0], str):\n",
    "# #             # Â≠óÁ¨¶‰∏≤ÂàóË°®ÔºàÂ¶Ç choicesÔºâ\n",
    "# #             avg_len = np.mean([len(v) for v in values if isinstance(v, list)])\n",
    "# #             print(f\"   - Âπ≥ÂùáÂàóË°®ÈïøÂ∫¶: {avg_len:.1f}\")\n",
    "# #             # ÊâìÂç∞ÂâçÂá†‰∏™Á§∫‰æã\n",
    "# #             example = values[0] if values[0] else []\n",
    "# #             print(f\"   - Á§∫‰æãÂÄº: {example[:3]}{'...' if len(example) > 3 else ''}\")\n",
    "            \n",
    "# #         elif isinstance(values[0], str):\n",
    "# #             # ÊñáÊú¨Â≠óÊÆµ\n",
    "# #             lengths = [len(v) for v in values if isinstance(v, str)]\n",
    "# #             if lengths:\n",
    "# #                 print(f\"   - ÊñáÊú¨ÈïøÂ∫¶: min={min(lengths)}, max={max(lengths)}, avg={np.mean(lengths):.1f}\")\n",
    "# #             example = next((v for v in values if v), \"\")\n",
    "# #             print(f\"   - Á§∫‰æãÂÄº: '{example[:100]}{'...' if len(example) > 100 else ''}'\")\n",
    "            \n",
    "# #         elif isinstance(values[0], (int, float)):\n",
    "# #             # Êï∞ÂÄºÂ≠óÊÆµ\n",
    "# #             nums = [v for v in values if isinstance(v, (int, float)) and v is not None]\n",
    "# #             if nums:\n",
    "# #                 print(f\"   - Êï∞ÂÄºËåÉÂõ¥: min={min(nums)}, max={max(nums)}, avg={np.mean(nums):.2f}\")\n",
    "                \n",
    "# #         else:\n",
    "# #             # ÂÖ∂‰ªñÁ±ªÂûãÔºàÂ¶Ç dictÔºâ\n",
    "# #             unique_types = set(type(v).__name__ for v in values)\n",
    "# #             print(f\"   - ÂÄºÁ±ªÂûã: {unique_types}\")\n",
    "# #             example = values[0] if values[0] is not None else \"None\"\n",
    "# #             print(f\"   - Á§∫‰æãÂÄº: {str(example)[:200]}{'...' if len(str(example)) > 200 else ''}\")\n",
    "        \n",
    "# #         print()  # Á©∫Ë°åÂàÜÈöî\n",
    "\n",
    "# # # ‰ΩøÁî®Á§∫‰æã\n",
    "# # dataset = load_dataset(\"AI4Math/MathVista\", split=\"testmini\")\n",
    "# # print_dataset_stats(dataset, num_samples=1000)\n",
    "\n",
    "\n",
    "\n",
    "# def count_images_in_decoded_image(decoded_image):\n",
    "#     \"\"\"\n",
    "#     Âà§Êñ≠ decoded_image ÊòØÂçïÂº†ÂõæËøòÊòØÂ§öÂº†ÂõæÔºåÂπ∂ËøîÂõûÊï∞Èáè„ÄÇ\n",
    "#     \"\"\"\n",
    "#     if decoded_image is None:\n",
    "#         return 0\n",
    "#     elif isinstance(decoded_image, Image.Image):\n",
    "#         return 1\n",
    "#     elif isinstance(decoded_image, list):\n",
    "#         # ËøáÊª§Êéâ None ÁöÑÊÉÖÂÜµÔºàËôΩÁÑ∂ MathVista ÈÄöÂ∏∏‰∏ç‰ºöÔºâ\n",
    "#         return len([img for img in decoded_image if img is not None])\n",
    "#     else:\n",
    "#         # ÁêÜËÆ∫‰∏ä‰∏ç‰ºöÂèëÁîüÔºå‰ΩÜÂÆâÂÖ®Ëµ∑ËßÅ\n",
    "#         print(f\"‚ö†Ô∏è Êú™Áü•Á±ªÂûã: {type(decoded_image)}\")\n",
    "#         return 0\n",
    "\n",
    "# # ÁªüËÆ°Êï¥‰∏™Êï∞ÊçÆÈõÜ\n",
    "# counts = []\n",
    "# for i in range(len(dataset)):\n",
    "#     img_field = dataset[i][\"decoded_image\"]\n",
    "#     n_imgs = count_images_in_decoded_image(img_field)\n",
    "#     counts.append(n_imgs)\n",
    "\n",
    "# # Ê±áÊÄªÁªüËÆ°\n",
    "# from collections import Counter\n",
    "# counter = Counter(counts)\n",
    "\n",
    "# print(\"üìä decoded_image ‰∏≠ÁöÑÂõæÁâáÊï∞ÈáèÂàÜÂ∏É:\")\n",
    "# for num_imgs, freq in sorted(counter.items()):\n",
    "#     print(f\"  - {num_imgs} Âº†Âõæ: {freq} ‰∏™Ê†∑Êú¨\")\n",
    "\n",
    "# # ÊâæÂá∫ÊâÄÊúâÂåÖÂê´ >1 Âº†ÂõæÁöÑÊ†∑Êú¨Á¥¢ÂºïÔºàÁî®‰∫é debugÔºâ\n",
    "# multi_img_indices = [i for i, n in enumerate(counts) if n > 1]\n",
    "# print(f\"\\nüîç ÂÖ±Êúâ {len(multi_img_indices)} ‰∏™Ê†∑Êú¨ÂåÖÂê´Â§öÂº†Âõæ„ÄÇ\")\n",
    "# if multi_img_indices:\n",
    "#     print(f\"Ââç 5 ‰∏™Â§öÂõæÊ†∑Êú¨Á¥¢Âºï: {multi_img_indices[:5]}\")\n",
    "#     # Êü•ÁúãÁ¨¨‰∏Ä‰∏™Â§öÂõæÊ†∑Êú¨\n",
    "#     idx = multi_img_indices[0]\n",
    "#     sample = dataset[idx]\n",
    "#     print(f\"Á§∫‰æãÈóÆÈ¢ò: {sample['question'][:100]}...\")\n",
    "#     print(f\"ÂõæÁâáÊï∞Èáè: {len(sample['decoded_image'])}\")\n",
    "#     for j, img in enumerate(sample['decoded_image']):\n",
    "#         print(f\"  Âõæ {j+1}: size={img.size}, mode={img.mode}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f10f2a-52e1-4e51-9a89-a288e703b1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400689f6-8bf9-4704-886a-f0cf1e6b2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASES = [\"read\", \"explore\", \"insight\", \"solve\", \"verify\"]\n",
    "\n",
    "@dataclass\n",
    "class ConfidenceSummary:\n",
    "    absolute: float\n",
    "    relative: float\n",
    "    phase_curve: List[Tuple[str, float]]\n",
    "\n",
    "@dataclass\n",
    "class SampleResult:\n",
    "    question_id: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    prediction: str\n",
    "    confidence: ConfidenceSummary\n",
    "    trajectory: List[Dict[str, float]]\n",
    "\n",
    "\n",
    "\n",
    "def build_prompt(question: str) -> str:\n",
    "    \"\"\"Áõ¥Êé•ËøîÂõû Qwen-VL Ë¶ÅÊ±ÇÁöÑ prompt Ê†ºÂºè\"\"\"\n",
    "    return (\n",
    "        \"You are a careful math and vision tutor. \"\n",
    "        \"Read the question and image, think step by step, \"\n",
    "        \"and output a concise final answer.\\n\"\n",
    "        \"<image>\\n\"  # ‚ö†Ô∏è ÂÖ≥ÈîÆÔºöÂøÖÈ°ªÂåÖÂê´ <image> Âç†‰ΩçÁ¨¶\n",
    "        f\"{question}\"\n",
    "    )\n",
    "\n",
    "def extract_confidence(\n",
    "    token_ids: List[int],\n",
    "    logprob_trace: List[Dict],\n",
    ") -> ConfidenceSummary:\n",
    "\n",
    "    token_confidences = []\n",
    "    margin_confidences = []\n",
    "\n",
    "    for step, token_id in enumerate(token_ids):\n",
    "        step_logprobs = logprob_trace[step] if step < len(logprob_trace) else {}\n",
    "        probs = []\n",
    "        target_prob = None\n",
    "\n",
    "        for entry in step_logprobs.values():\n",
    "            if isinstance(entry, (float, int)):\n",
    "                logprob = float(entry)\n",
    "                entry_token_id = None\n",
    "            else:\n",
    "                logprob = getattr(entry, \"logprob\", None)\n",
    "                entry_token_id = getattr(entry, \"token_id\", None)\n",
    "\n",
    "            if logprob is None:\n",
    "                continue\n",
    "\n",
    "            prob = float(torch.exp(torch.tensor(logprob)).item())\n",
    "            probs.append(prob)\n",
    "            if entry_token_id == token_id:\n",
    "                target_prob = prob\n",
    "\n",
    "        if target_prob is None and probs:\n",
    "            target_prob = probs[0]\n",
    "\n",
    "        token_confidences.append(target_prob or 0.0)\n",
    "\n",
    "        if len(probs) >= 2:\n",
    "            top_two = sorted(probs, reverse=True)[:2]\n",
    "            margin_confidences.append(top_two[0] - top_two[1])\n",
    "        elif probs:\n",
    "            margin_confidences.append(probs[0])\n",
    "        else:\n",
    "            margin_confidences.append(0.0)\n",
    "\n",
    "    absolute = float(sum(token_confidences) / max(len(token_confidences), 1))\n",
    "    relative = float(sum(margin_confidences) / max(len(margin_confidences), 1))\n",
    "    phase_curve = interpolate_phases(token_confidences)\n",
    "\n",
    "    return ConfidenceSummary(absolute, relative, phase_curve)\n",
    "\n",
    "\n",
    "def interpolate_phases(token_confidences: List[float]) -> List[Tuple[str, float]]:\n",
    "    if not token_confidences:\n",
    "        return [(phase, 0.0) for phase in PHASES]\n",
    "\n",
    "    mn, mx = min(token_confidences), max(token_confidences)\n",
    "    span = max(mx - mn, 1e-6)\n",
    "\n",
    "    normalized = [(c - mn) / span for c in token_confidences]\n",
    "\n",
    "    checkpoints = []\n",
    "    for idx, phase in enumerate(PHASES):\n",
    "        pos = int((idx / max(len(PHASES)-1, 1)) * (len(normalized)-1))\n",
    "        checkpoints.append((phase, float(normalized[pos])))\n",
    "\n",
    "    return checkpoints\n",
    "\n",
    "def run_sample(llm, processor, sample, sampling_params):\n",
    "    # ‰ªéË∑ØÂæÑÂä†ËΩΩÂõæÂÉèÔºà‰Ω†ÂΩìÂâçÁî®ÁöÑÊòØÂéüÂßã image Ë∑ØÂæÑÔºå‰∏çÊòØ decoded_imageÔºâ\n",
    "    image = Image.open(\"/root/autodl-tmp/data/\" + sample[\"image\"]).convert(\"RGB\")\n",
    "    \n",
    "    # ÊûÑÈÄ† OpenAI-style messagesÔºàvLLM ÂÜÖÈÉ®‰ºöËá™Âä®ËΩ¨Êç¢‰∏∫ÂêàÊ≥ï tokenÔºâ\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": sample[\"query\"]}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # ‚úÖ ÂÖ≥ÈîÆÔºöÁî® processor ÁîüÊàê promptÔºàÂåÖÂê´ÂêàÊ≥ïÁöÑËßÜËßâ tokenÔºâ\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Ë∞ÉÁî® vLLMÔºàÊ≥®ÊÑèÔºömulti_modal_data ‰º† [image] ÂàóË°®ÔºÅÔºâ\n",
    "    outputs = llm.generate(\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"multi_modal_data\": {\"image\": [image]}  \n",
    "        },\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    print(type(outputs))\n",
    "    print(outputs)\n",
    "    \n",
    "    result = outputs[0].outputs[0]\n",
    "    print(typr(result))\n",
    "    print(result)\n",
    "    \n",
    "    logprob_trace = result.logprobs or []\n",
    "    confidence = extract_confidence(result.token_ids, logprob_trace)\n",
    "\n",
    "    return SampleResult(\n",
    "        question_id=str(sample.get(\"pid\", sample.get(\"id\", \"unknown\"))),\n",
    "        question=sample[\"question\"],\n",
    "        answer=sample.get(\"answer\", \"\"),\n",
    "        prediction=result.text.strip(),\n",
    "        confidence=confidence,\n",
    "        trajectory=[\n",
    "            {\"phase\": phase, \"confidence\": value}\n",
    "            for phase, value in confidence.phase_curve\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "def log_to_wandb(table, result):\n",
    "    table.add_data(\n",
    "        result.question_id,\n",
    "        result.question,\n",
    "        result.answer,\n",
    "        result.prediction,\n",
    "        result.confidence.absolute,\n",
    "        result.confidence.relative,\n",
    "        result.trajectory,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb48af-95ba-4a96-b76a-fee4fd528c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 17:41:25 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 8192, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'enable_chunked_prefill': False, 'model': '/root/autodl-tmp/model'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 17:41:25 [model.py:547] Resolved architecture: Qwen3VLForConditionalGeneration\n",
      "WARNING 12-10 17:41:25 [model.py:1733] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 12-10 17:41:25 [model.py:1510] Using max model len 8192\n",
      "INFO 12-10 17:41:25 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12775)\u001b[0;0m INFO 12-10 17:41:26 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12775)\u001b[0;0m INFO 12-10 17:41:26 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/root/autodl-tmp/model', speculative_config=None, tokenizer='/root/autodl-tmp/model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/autodl-tmp/model, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=12775)\u001b[0;0m ERROR 12-10 17:41:26 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12775)\u001b[0;0m INFO 12-10 17:41:27 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12775)\u001b[0;0m WARNING 12-10 17:41:27 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"/root/autodl-tmp/model\"\n",
    "split = \"testmini\"\n",
    "limit = 32\n",
    "batch_size = 8\n",
    "max_new_tokens = 1024\n",
    "project = \"StateReasoning\"\n",
    "run_name = \"qwen3-vl-4b-thinking\"\n",
    "data_dir = None\n",
    "# V100 (compute capability 7.0) does not support bfloat16; use float16 instead\n",
    "# to avoid engine initialization failure.\n",
    "dtype = \"float16\"\n",
    "\n",
    "llm = LLM(\n",
    "        model=model_id,\n",
    "        trust_remote_code=True,\n",
    "        dtype=dtype,\n",
    "        # Limit context length to fit V100 32GB KV cache (was 262144 needing ~36 GiB)\n",
    "        max_model_len=8192,\n",
    "        # Slightly raise utilization; adjust if OOM\n",
    "        gpu_memory_utilization=0.5,\n",
    "        enable_chunked_prefill=False  # Â§öÊ®°ÊÄÅÂª∫ËÆÆÂÖ≥Èó≠\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02994098-cf43-4454-a138-12439e0ead26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = Image.open(\"/root/autodl-tmp/data/images/4.jpg\").convert(\"RGB\")\n",
    "\n",
    "# ÊûÑÈÄ† OpenAI-style messagesÔºàvLLM ÂÜÖÈÉ®‰ºöËá™Âä®ËΩ¨Êç¢‰∏∫ÂêàÊ≥ï tokenÔºâ\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Subtract all large rubber spheres. Subtract all big shiny cylinders. How many objects are left?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# ‚úÖ ÂÖ≥ÈîÆÔºöÁî® processor ÁîüÊàê promptÔºàÂåÖÂê´ÂêàÊ≥ïÁöÑËßÜËßâ tokenÔºâ\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Ë∞ÉÁî® vLLMÔºàÊ≥®ÊÑèÔºömulti_modal_data ‰º† [image] ÂàóË°®ÔºÅÔºâ\n",
    "outputs = llm.generate(\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": [image]}  \n",
    "    },\n",
    "    sampling_params=sampling_params,\n",
    ")\n",
    "print(type(outputs))\n",
    "print(outputs)\n",
    "\n",
    "result = outputs[0].outputs[0]\n",
    "print(typr(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2accddb0-5e63-4e4b-b636-aaf7b7f41c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/root/autodl-tmp/model\"\n",
    "split = \"testmini\"\n",
    "limit = 32\n",
    "batch_size = 8\n",
    "max_new_tokens = 1024\n",
    "project = \"StateReasoning\"\n",
    "run_name = \"qwen3-vl-4b-thinking\"\n",
    "data_dir = None\n",
    "# V100 (compute capability 7.0) does not support bfloat16; use float16 instead\n",
    "# to avoid engine initialization failure.\n",
    "dtype = \"float16\"\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    llm = LLM(\n",
    "        model=model_id,\n",
    "        trust_remote_code=True,\n",
    "        dtype=dtype,\n",
    "        # Limit context length to fit V100 32GB KV cache (was 262144 needing ~36 GiB)\n",
    "        max_model_len=8192,\n",
    "        # Slightly raise utilization; adjust if OOM\n",
    "        gpu_memory_utilization=0.5,\n",
    "        enable_chunked_prefill=False  # Â§öÊ®°ÊÄÅÂª∫ËÆÆÂÖ≥Èó≠\n",
    "    )\n",
    "\n",
    "\n",
    "    dataset = load_dataset(\"AI4Math/MathVista\", split=split)\n",
    "    if limit:\n",
    "        dataset = dataset.select(range(limit))\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=0.1,\n",
    "        logprobs=10,\n",
    "    )\n",
    "\n",
    "    wandb.init(project=project, name=run_name)\n",
    "\n",
    "    table = wandb.Table(\n",
    "        columns=[\n",
    "            \"question_id\",\n",
    "            \"question\",\n",
    "            \"ground_truth\",\n",
    "            \"prediction\",\n",
    "            \"absolute_confidence\",\n",
    "            \"relative_confidence\",\n",
    "            \"phase_trajectory\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for start in tqdm(range(0, len(dataset), batch_size), desc=\"Evaluating\"):\n",
    "\n",
    "        end = min(start + batch_size, len(dataset))\n",
    "        batch = dataset.select(range(start, end)).to_list()\n",
    "\n",
    "        for sample in batch:\n",
    "            result = run_sample(llm, processor, sample, sampling_params)\n",
    "            log_to_wandb(table, result)\n",
    "\n",
    "    wandb.log({\"mathvista_eval\": table})\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e511ac-a3ad-4849-8ee5-f024caa37c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 16:46:47 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 8192, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'enable_chunked_prefill': False, 'model': '/root/autodl-tmp/model'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 16:46:47 [model.py:547] Resolved architecture: Qwen3VLForConditionalGeneration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-10 16:46:47 [model.py:1733] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 12-10 16:46:47 [model.py:1510] Using max model len 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 16:46:48,027\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 16:46:48 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:46:51 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:46:51 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/root/autodl-tmp/model', speculative_config=None, tokenizer='/root/autodl-tmp/model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/autodl-tmp/model, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m ERROR 12-10 16:46:51 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:46:51 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m WARNING 12-10 16:46:52 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:46:55 [gpu_model_runner.py:2602] Starting to load model /root/autodl-tmp/model...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:46:56 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:46:56 [cuda.py:372] Using FlexAttention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.02s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.42s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:47:01 [default_loader.py:267] Loading weights took 5.08 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:47:02 [gpu_model_runner.py:2653] Model loading took 8.6789 GiB and 5.396256 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:47:02 [gpu_model_runner.py:3344] Encoder cache will be initialized with a budget of 151250 tokens, and profiled with 1 video items of the maximum feature size.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:47:25 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/ea218c08a9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:47:25 [backends.py:559] Dynamo bytecode transform time: 11.05 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:47:32 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:48:17 [backends.py:218] Compiling a graph for dynamic shape takes 50.64 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:48:31 [monitor.py:34] torch.compile takes 61.69 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:48:33 [gpu_worker.py:298] Available KV cache memory: 2.47 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:48:33 [kv_cache_utils.py:1087] GPU KV cache size: 17,968 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:48:33 [kv_cache_utils.py:1091] Maximum concurrency for 8,192 tokens per request: 2.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m WARNING 12-10 16:48:33 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67/67 [00:05<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:48:40 [gpu_model_runner.py:3480] Graph capturing finished in 7 secs, took 0.56 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=9931)\u001b[0;0m INFO 12-10 16:48:40 [core.py:210] init engine (profile, create kv cache, warmup model) took 98.49 seconds\n",
      "INFO 12-10 16:48:42 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleofyfan\u001b[0m (\u001b[33mleofyfan-east-china-normal-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/projects/StateReasoning/wandb/run-20251210_164848-nto26nmt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leofyfan-east-china-normal-university/StateReasoning/runs/nto26nmt' target=\"_blank\">qwen3-vl-4b-thinking</a></strong> to <a href='https://wandb.ai/leofyfan-east-china-normal-university/StateReasoning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leofyfan-east-china-normal-university/StateReasoning' target=\"_blank\">https://wandb.ai/leofyfan-east-china-normal-university/StateReasoning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leofyfan-east-china-normal-university/StateReasoning/runs/nto26nmt' target=\"_blank\">https://wandb.ai/leofyfan-east-china-normal-university/StateReasoning/runs/nto26nmt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.40s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:57<00:00, 117.32s/it, est. speed input: 10.93 toks/s, output: 8.73 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:57<00:00, 117.32s/it, est. speed input: 10.93 toks/s, output: 8.73 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 28.78it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:02<00:00, 62.51s/it, est. speed input: 13.26 toks/s, output: 16.38 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:02<00:00, 62.52s/it, est. speed input: 13.26 toks/s, output: 16.38 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 99.36it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:35<00:00, 35.87s/it, est. speed input: 5.07 toks/s, output: 28.55 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:35<00:00, 35.87s/it, est. speed input: 5.07 toks/s, output: 28.55 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 117.97it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:30<00:00, 30.90s/it, est. speed input: 4.66 toks/s, output: 33.14 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:30<00:00, 30.90s/it, est. speed input: 4.66 toks/s, output: 33.14 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 107.87it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:30<00:00, 30.97s/it, est. speed input: 5.07 toks/s, output: 33.06 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:30<00:00, 30.97s/it, est. speed input: 5.07 toks/s, output: 33.06 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 142.77it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:32<00:00, 32.15s/it, est. speed input: 5.82 toks/s, output: 31.85 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:32<00:00, 32.15s/it, est. speed input: 5.82 toks/s, output: 31.85 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 119.30it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:13<00:00, 13.16s/it, est. speed input: 12.16 toks/s, output: 38.30 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:13<00:00, 13.16s/it, est. speed input: 12.16 toks/s, output: 38.30 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 73.70it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:34<00:00, 34.72s/it, est. speed input: 11.12 toks/s, output: 29.49 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:34<00:00, 34.72s/it, est. speed input: 11.12 toks/s, output: 29.49 toks/s]\u001b[A\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 1/4 [06:02<18:06, 362.18s/it]\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 133.21it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:31<00:00, 31.87s/it, est. speed input: 5.87 toks/s, output: 32.13 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:31<00:00, 31.88s/it, est. speed input: 5.87 toks/s, output: 32.13 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 75.99it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:31<00:00, 31.42s/it, est. speed input: 7.13 toks/s, output: 32.59 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:31<00:00, 31.42s/it, est. speed input: 7.13 toks/s, output: 32.59 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 154.53it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:31<00:00, 31.53s/it, est. speed input: 4.54 toks/s, output: 32.48 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:31<00:00, 31.53s/it, est. speed input: 4.54 toks/s, output: 32.48 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 79.13it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:20<00:00, 20.55s/it, est. speed input: 17.52 toks/s, output: 31.44 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:20<00:00, 20.55s/it, est. speed input: 17.52 toks/s, output: 31.44 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 100.23it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:33<00:00, 33.53s/it, est. speed input: 7.81 toks/s, output: 30.54 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:33<00:00, 33.53s/it, est. speed input: 7.81 toks/s, output: 30.54 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 82.94it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:36<00:00, 36.71s/it, est. speed input: 10.27 toks/s, output: 27.89 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:36<00:00, 36.72s/it, est. speed input: 10.27 toks/s, output: 27.89 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 82.18it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:30<00:00, 30.85s/it, est. speed input: 5.22 toks/s, output: 33.19 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:30<00:00, 30.85s/it, est. speed input: 5.22 toks/s, output: 33.19 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 92.57it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:33<00:00, 33.38s/it, est. speed input: 7.70 toks/s, output: 30.67 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:33<00:00, 33.39s/it, est. speed input: 7.70 toks/s, output: 30.67 toks/s]\u001b[A\n",
      "Evaluating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [10:13<09:53, 296.75s/it]\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 125.22it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:32<00:00, 32.51s/it, est. speed input: 5.20 toks/s, output: 31.50 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:32<00:00, 32.51s/it, est. speed input: 5.20 toks/s, output: 31.50 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 77.41it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:09<00:00,  9.57s/it, est. speed input: 34.27 toks/s, output: 33.64 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:09<00:00,  9.57s/it, est. speed input: 34.27 toks/s, output: 33.64 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 34.39it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:34<00:00, 34.28s/it, est. speed input: 21.21 toks/s, output: 24.42 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:34<00:00, 34.28s/it, est. speed input: 21.21 toks/s, output: 24.42 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.94it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:27<00:00, 27.11s/it, est. speed input: 21.32 toks/s, output: 28.04 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:27<00:00, 27.11s/it, est. speed input: 21.32 toks/s, output: 28.04 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 116.43it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.62s/it, est. speed input: 58.79 toks/s, output: 38.18 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.62s/it, est. speed input: 58.79 toks/s, output: 38.18 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.75it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.02s/it, est. speed input: 80.88 toks/s, output: 33.55 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.02s/it, est. speed input: 80.88 toks/s, output: 33.55 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 76.89it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:36<00:00, 36.59s/it, est. speed input: 7.65 toks/s, output: 27.98 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:36<00:00, 36.60s/it, est. speed input: 7.65 toks/s, output: 27.98 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.51it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.77s/it, est. speed input: 85.33 toks/s, output: 36.04 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.78s/it, est. speed input: 85.33 toks/s, output: 36.04 toks/s]\u001b[A\n",
      "Evaluating:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [12:46<03:51, 231.23s/it]\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 74.31it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:11<00:00, 11.26s/it, est. speed input: 30.92 toks/s, output: 34.29 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:11<00:00, 11.26s/it, est. speed input: 30.92 toks/s, output: 34.29 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 77.60it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.89s/it, est. speed input: 120.25 toks/s, output: 36.39 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.89s/it, est. speed input: 120.25 toks/s, output: 36.39 toks/s]\u001b[A\n",
      "\n",
      "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.58it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:47<00:00, 107.55s/it, est. speed input: 42.52 toks/s, output: 9.52 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:47<00:00, 107.56s/it, est. speed input: 42.52 toks/s, output: 9.52 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.51it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:26<00:00, 26.16s/it, est. speed input: 11.81 toks/s, output: 29.62 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:26<00:00, 26.17s/it, est. speed input: 11.81 toks/s, output: 29.62 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.75it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:18<00:00, 18.17s/it, est. speed input: 19.65 toks/s, output: 30.43 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:18<00:00, 18.17s/it, est. speed input: 19.65 toks/s, output: 30.43 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 112.11it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:33<00:00, 33.12s/it, est. speed input: 5.74 toks/s, output: 30.92 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:33<00:00, 33.12s/it, est. speed input: 5.74 toks/s, output: 30.92 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 76.22it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:23<00:00, 23.61s/it, est. speed input: 15.80 toks/s, output: 30.28 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:23<00:00, 23.62s/it, est. speed input: 15.80 toks/s, output: 30.28 toks/s]\u001b[A\n",
      "\n",
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 123.43it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:34<00:00, 34.27s/it, est. speed input: 5.14 toks/s, output: 29.88 toks/s]\u001b[A\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:34<00:00, 34.27s/it, est. speed input: 5.14 toks/s, output: 29.88 toks/s]\u001b[A\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [17:04<00:00, 256.16s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">qwen3-vl-4b-thinking</strong> at: <a href='https://wandb.ai/leofyfan-east-china-normal-university/StateReasoning/runs/nto26nmt' target=\"_blank\">https://wandb.ai/leofyfan-east-china-normal-university/StateReasoning/runs/nto26nmt</a><br> View project at: <a href='https://wandb.ai/leofyfan-east-china-normal-university/StateReasoning' target=\"_blank\">https://wandb.ai/leofyfan-east-china-normal-university/StateReasoning</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251210_164848-nto26nmt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a89b82-66ad-4519-8126-c01ca6139444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sr",
   "language": "python",
   "name": "sr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
